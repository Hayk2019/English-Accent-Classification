import argparse
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
from tensorflow.keras.regularizers import l2
import tensorflow as tf
import json
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö
class NPYDataGenerator(Sequence):
    def __init__(self, X, y, batch_size=32, shuffle=True):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexes = np.arange(len(self.X))
        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.X) / self.batch_size))

    def __getitem__(self, index):
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        X_batch = self.X[batch_indexes]
        y_batch = self.y[batch_indexes]
        return X_batch, y_batch

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indexes)

# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏
def build_model(input_shape, num_classes):
    model = Sequential([
        Input(shape=input_shape),
        Conv2D(32, (3, 3), activation='relu',kernel_regularizer=l2(0.001)),
        MaxPooling2D((2, 2)),

        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),

        Flatten(),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    return model

def main(args):
    print("\U0001F4E6 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    X = np.load(args.x_path).astype('float32')
    y = np.load(args.y_path)
    label_classes = np.load(args.classes_path)

    print("‚úÖ –§–æ—Ä–º—ã –º–∞—Å—Å–∏–≤–æ–≤: X =", X.shape, ", y =", y.shape)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã
    train_gen = NPYDataGenerator(X_train, y_train, batch_size=args.batch_size)
    val_gen = NPYDataGenerator(X_test, y_test, batch_size=args.batch_size)

    print("\U0001F9E0 –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å...")
    model = build_model(input_shape=X.shape[1:], num_classes=y.shape[1])
    model.compile(optimizer=Adam(learning_rate=args.lr),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    print("üöÄ –û–±—É—á–µ–Ω–∏–µ...")
    history = model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=args.epochs,
        callbacks=[early_stop]
    )

    print("üìÄ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤", args.model_path)
    model.save(args.model_path)

    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –≤ training_history.json")
    with open("training_history.json", "w") as f:
        json.dump(history.history, f)

    print("\U0001F5C3 –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏...")
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
    plt.title('üìà Accuracy per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
    plt.title('üìâ Loss per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("training_plots.png")
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ training_plots.png")

    print("‚úÖ –ì–æ—Ç–æ–≤–æ! –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ:")
    loss, acc = model.evaluate(val_gen)
    print(f"üéØ Accuracy: {acc:.4f}")

    # –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    sample = X_test[0:1]
    pred = model.predict(sample)
    predicted_index = np.argmax(pred)
    print("üîç –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: ", label_classes[predicted_index])

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ CNN –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∞–∫—Ü–µ–Ω—Ç–∞")
    parser.add_argument("--x_path", type=str, default="X_melspec.npy", help="–ü—É—Ç—å –∫ X (–º–µ–ª—Å–ø–µ–∫—Ç—Ä—ã)")
    parser.add_argument("--y_path", type=str, default="y_onehot.npy", help="–ü—É—Ç—å –∫ y (one-hot –º–µ—Ç–∫–∏)")
    parser.add_argument("--classes_path", type=str, default="label_classes.npy", help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–ª–∞—Å—Å–æ–≤")
    parser.add_argument("--model_path", type=str, default="cnn_accent_model.h5", help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å")
    parser.add_argument("--epochs", type=int, default=30, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument("--batch_size", type=int, default=32, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--lr", type=float, default=0.001, help="Learning rate")

    args = parser.parse_args()
    main(args)

